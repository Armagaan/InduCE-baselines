{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining CFGNN's Treecycles dataset using PGExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd9eb6b06b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "from torch_geometric.explain import Explainer\n",
    "from torch_geometric.explain.algorithm import PGExplainer\n",
    "from torch_geometric.explain.config import ModelConfig\n",
    "from torch_geometric.explain.metric import fidelity\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "from pyg_gcn import GNN\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/gnn_explainer/syn4.pickle\", \"rb\") as file:\n",
    "\tdata = pickle.load(file)\n",
    "adj = torch.tensor(data[\"adj\"], dtype=torch.float).squeeze()\n",
    "features = torch.tensor(data[\"feat\"], dtype=torch.float).squeeze()\n",
    "labels = torch.tensor(data[\"labels\"], dtype=torch.long).squeeze()\n",
    "idx_train = torch.tensor(data[\"train_idx\"], dtype=torch.long)\n",
    "with open(f\"../data/eval-sets/syn4.pickle\", \"rb\") as file:\n",
    "\tidx_test = torch.tensor(pickle.load(file), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = adj.to_sparse().indices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN(\n",
       "  (gc1): GCNConv(10, 20)\n",
       "  (gc2): GCNConv(20, 20)\n",
       "  (gc3): GCNConv(20, 20)\n",
       "  (lin): Linear(in_features=60, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GNN(\n",
    "    nfeat=features.size(1),\n",
    "    nhid=20,\n",
    "    nout=20,\n",
    "    nclass=1 + labels.max().item(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "state_dict = torch.load(\"../models/gcn_3layer_syn4.pt\")\n",
    "for key in list(state_dict):\n",
    "    if \"gc\" in key and \"weight\" in key:\n",
    "        new_key = key.split(\".\")\n",
    "        new_key = new_key[0] + \".lin.\" + new_key[1]\n",
    "        state_dict[new_key] = state_dict[key].T\n",
    "        del state_dict[key]\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* The pyg version seems replicate the results without normalizing the adjacency.\n",
    "output = model(features, edge_index)\n",
    "pred = output.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 91.24%\n",
      "Test accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "train_acc = 100 * (pred[idx_train] == labels[idx_train]).sum() / pred[idx_train].size(0)\n",
    "test_acc = 100 * (pred[idx_test] == labels[idx_test]).sum() / pred[idx_test].size(0)\n",
    "print(\n",
    "    f\"Training accuracy: {train_acc:.2f}%\",\n",
    "    f\"Test accuracy: {test_acc:.2f}%\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "LR = 0.003\n",
    "TOP_K = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=PGExplainer(epochs=EPOCHS, lr=LR),\n",
    "    explanation_type=\"phenomenon\",\n",
    "    edge_mask_type=\"object\",\n",
    "    model_config=ModelConfig(\n",
    "        mode=\"multiclass_classification\",\n",
    "        task_level=\"node\",\n",
    "        return_type=\"log_probs\"\n",
    "    ),\n",
    "    threshold_config=dict(threshold_type='topk', value=TOP_K),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for index in idx_train:\n",
    "        loss = explainer.algorithm.train(\n",
    "            epoch=epoch,\n",
    "            model=model,\n",
    "            x=features,\n",
    "            edge_index=edge_index,\n",
    "            target=labels,\n",
    "            index=index.item()\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.38 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "start = perf_counter()\n",
    "for index in idx_test:\n",
    "    explanation = explainer(features, edge_index, target=labels, index=index.item())\n",
    "end = perf_counter()\n",
    "print(\"Time elapsed:\", round(end - start, 2), \"seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_fidelity(indices):\n",
    "    fidelities = list()\n",
    "    for index in indices:\n",
    "        explanation = explainer(features, edge_index, target=labels, index=index.item())\n",
    "        fidelities.append(fidelity(explainer, explanation)[0])\n",
    "    fidelities = torch.tensor(fidelities, dtype=float)\n",
    "    return 1 - fidelities.mean(), fidelities.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fidelity_mean_train, fidelity_std_train = cal_fidelity(idx_train)\n",
    "fidelity_mean_test, fidelity_std_test = cal_fidelity(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test fidelity: 0.3472, std=0.4794\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Average training fidelity: {fidelity_mean_train:.4f}, std={fidelity_std_train:.4f}\")\n",
    "print(f\"Average test fidelity: {fidelity_mean_test:.4f}, std={fidelity_std_test:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average explanaiton size: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average explanaiton size: {TOP_K}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(indices):\n",
    "    accuracies = list()\n",
    "    for index in indices:\n",
    "        explanation = explainer(features, edge_index, target=labels, index=index.item())\n",
    "        # Find all edges involved in the explanation.\n",
    "        edges_involved = explanation.edge_index[:, explanation.edge_mask.to(bool)]\n",
    "        # Count the edges where the src and the dest have non-zero labels.\n",
    "        sources = edges_involved[0]\n",
    "        destinations = edges_involved[1]\n",
    "        acc = 0\n",
    "        for i in range(edges_involved.size(1)):\n",
    "            if labels[sources[i]] != 0 and labels[destinations[i]] != 0:\n",
    "                acc += 1\n",
    "        # Divide by the total edges involved.\n",
    "        acc /= edges_involved.size(1)\n",
    "        accuracies.append(acc)\n",
    "    # Divide by the #instances\n",
    "    accuracies = torch.tensor(accuracies, dtype=float)\n",
    "    return accuracies.mean(), accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_mean_train, acc_std_train = cal_acc(idx_train)\n",
    "acc_mean_test, acc_std_test = cal_acc(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy: 0.7685, std=0.2791\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Average train accuracy: {acc_mean_train:.4f}, std={acc_std_train:.4f}\")\n",
    "print(f\"Average test accuracy: {acc_mean_test:.4f}, std={acc_std_test:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sparsity(indices, is_undirected:bool = True):\n",
    "    sparsity_t = list()\n",
    "    # extract the subgraph\n",
    "    for node_index in indices:\n",
    "        __, __, __, edge_mask = k_hop_subgraph(\n",
    "            node_idx=node_index.item(),\n",
    "            num_hops=PGExplainer._num_hops(model),\n",
    "            edge_index=edge_index,\n",
    "            num_nodes=features.size(0),\n",
    "            flow=PGExplainer._flow(model),\n",
    "        )\n",
    "        # find the number of edges in the subgraph.\n",
    "        num_edges = edge_mask.nonzero().size(0)\n",
    "        # account for undirected edges.\n",
    "        if is_undirected:\n",
    "            num_edges = ceil(num_edges / 2)\n",
    "        # Find all edges involved in the explanation.\n",
    "        explanation = explainer(features, edge_index, target=labels, index=index.item())\n",
    "        edges_involved = explanation.edge_index[:, explanation.edge_mask.to(bool)]\n",
    "        explanation_size = edges_involved.size(1)\n",
    "        if explanation_size != 6:\n",
    "            print(f\"{node_index}: {explanation_size}: {num_edges}\")\n",
    "        sparsity = 1 - (explanation_size / num_edges)\n",
    "        sparsity_t.append(sparsity)\n",
    "    sparsity_t = torch.tensor(sparsity_t, dtype=float)\n",
    "    return sparsity_t.mean(), sparsity_t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity_mean_train, sparsity_std_train = cal_sparsity(idx_train, True)\n",
    "sparsity_mean_test, sparsity_std_test = cal_sparsity(idx_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test sparsity: 0.3388, std=0.1840\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Average train sparsity: {sparsity_mean_train:.4f}, std={sparsity_std_train:.4f}\")\n",
    "print(f\"Average test sparsity: {sparsity_mean_test:.4f}, std={sparsity_std_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg-2.3.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
